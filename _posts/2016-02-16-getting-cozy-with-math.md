# Why I wrote these notes
I remember sitting in my family living room around in the Spring time a few years ago, skimming through university websites and trying to decide what I wanted to go off and study. I was pretty convinced that Computer Science was the only way to go, but as I began to look into requirements for most of the programs I'd be applying to, I caught on to a trend I wasn't too keen on. 

No matter where I looked, Computer Science seemed inescapebly linked to math.

Before long, I'd stepped away from the applications and was dropping into links like these:

01. [https://www.quora.com/Should-I-pursue-computer-science-if-I-am-bad-at-math](https://www.quora.com/Should-I-pursue-computer-science-if-I-am-bad-at-math)

02. [http://www.computersciencedegreehub.com/faq/computer-science-degree-require-lot-math-courses/](http://www.computersciencedegreehub.com/faq/computer-science-degree-require-lot-math-courses/)

03. [http://www.careerigniter.com/questions/do-i-need-to-be-good-at-math-to-be-a-computer-programmer/](http://www.careerigniter.com/questions/do-i-need-to-be-good-at-math-to-be-a-computer-programmer/)

04. [https://www.youtube.com/watch?v=AziNzSvPZ-0](https://www.youtube.com/watch?v=AziNzSvPZ-0)


Don't get me wrong, I'd taken all the required high school math courses and I'd done quite well in them. Getting into a university wasn't worrying me - committing myself to something that I found extremely difficult was.

But I went through with it, and I don't regret it today. Let me explain:

When I first committed myself to Computer Science, I developed a sort of personal contract by which I would complete the bare minimum amount of math courses I needed to get the degree, and spend the rest of the time focused on programming. I was pretty happy with that plan until the fifth minute of my first class in Calculus, when my teacher shattered all hopes of it ever working.

There are three first year Calc courses at the University of Toronto, which essentially break down like this:

- One of them is entirely computational, and makes no reference to proofs (i.e. my obvious choice at this point in the story)
- One of them is mostly proof based, with some computation
- One of them is entirely theoretical with little to no mention of actual computation

My instructor told us that if we were studying Computer Science, we'd be better off in one of the latter two courses. I spoke to him after to ask why that was the case, and he explained that most upper year CS courses were essentially math courses.

So here I was, listening to someone tell me that I was making a mistake. I had no real sense of what Computer Science entailed, so I had no understanding of what he meant. At this point I had a more serious problem, because now it was too late to turn back. Of course, I didn't *want* to switch into something more involved. I knew from high school that math could take me a very long time, and that I'd probably be wasting many, many hours on problem sets. On top of that, I still worried that I wouldn't retain or truly understand the material. To this day I can still be hilariously bad at mental math, and at that time I didn't believe that I was ever really going to improve at all. I'd spent a whole year doing Calculus prior, and I couldn't remember what first principles were. To make things worse, as a few weeks went by, I'd heard from my roommate that the other course was proving to be extremely difficult.

Despite my instructors advice, I stayed in that class for the whole year, and it wasn't particularly difficult. It still took me quite some time, but that was mostly because computations can be quite dry. One might say that I'd caught a lucky break. The real difficulties began in my second semester when I took a mandatory course in Discreet Mathematics - the same one which inspired these notes. 

At first, it seemed to be pretty promising; here's a direct quote from the [course notes](http://www.cs.toronto.edu/~gfb/csc165/2015F/CSC165.2015W.Notes.pdf):

*You need this course if you DO:*

- *Memorize math*
- *Have trouble explaining what you are doing in a mathematical or technical question*
- *Have trouble understanding word problems*

*You need this course if you DONT*

- *Like reading math textbooks to learn new math*
- *Enjoy talking about abstract x and y just as much as when concrete examples are given for them*

Every one of those points had described me, and I wasn't even a page in yet. Needless to say, I thought I'd finally found a course that might help me improve. In short, it *did* help me a *lot*, but man, did it hurt to get there.

Like I said, things went well at first: it turns out that I'm pretty good with formal logic and set theory. It was mostly around the fourth week, when we finally got to proofs, that things started to take a turn for the worst. From that point on, that course became a total nightmare. I spent countless hours trying to understand the material and finish the assignments, often to what felt like little avail. I met a few other friends who were struggling as well, and we ended up working through it together. I needed to get a good mark in the course to be finally admitted into the program itself, so I felt quite a bit of pressure.

I managed to do quite well on most of the assignments + midterms, but I felt like I was relying too much on my friends. It's easy, when you work on something difficult with others, to feel like you aren't really pulling your weight. We'd been working together on almost everything, and I still wasn't convinced that I could stand firmly on my own two feet yet. Most of the course had focussed on different standard proof techniques, with a bit of computaional analysis. When it came time for the exam, I froze up as I came across questions I had no idea how to solve. I walked out hoping I could get by with a half-decent mark to fit the program requirements.

I managed to pull off an A. That might not be an outstanding mark or anything, but it definitely felt like one to me - I couldn't believe it. I'd worked so hard and it had actually payed off. What's more, once I'd finally calmed down, I realized that I had actually started to get a better sense for solving mathematical problems (something I was starting to notice in my Calc class). I wasn't dealing with boring computations anymore but puzzles, and my perspective on the matter was shifting from one where I simply absorbed facts to one where I sought to understand what was going on, intuitively.

Here are a few things that, in retrospect, I learned from that course:

1. Most people have a (very) hard time with their first exposure to pure mathematics.
2. That course wasn't actually hard... to anyone comfortable with proofs. It was challenging for someone who had never seen them before.
4. Once you start trying to understand why theorems are true intuitively, not only do they become more interesting, but they become much easier to work with.

I still didn't fully become comfortable with proofs until midway through the follow-up course in my second year. To be honest, the only reason I became comfortabe with the material was because I practiced. The more you read (good) proofs, think intuitively about why they are true, and attempt to prove things on your own, the better you will get. You will fail a lot, do that as quickly as possible. The more you learn doesn't work, the more you'll begin to get a sense for what does work. Baby ideas, such as the formal definition of an odd/even number, will come back. If you understand these things, you will understand more complicated things.

It took me a while to get used to proofs, and "real" mathematics, but now that I am I couldn't be happier. Even though they're usually the most challenging, my courses in discreet math and analysis are always my favorites. Eventually a breakpoint will come, and you'll solve something. It'll feel really really good, and you'll want to do it again. Don't worry if it seems impossible at first, it does to most of us. If you're really willing to put in the time and get at it, it'll pull through. So, if you feel like I did, just remember that I used to think I was awful in math, and now I'm routinely beating the class average.

Now, while I quite like the professors in my department, the course notes I refered to earlier are quite confusing with respects to the material they try to cover. So, I've compiled these notes as my own attempt to explain things in simpler terms. Since I struggled so much with them, I feel like I may be able to put those concepts in a way that reaches people more easily. We'll see, I won't get ahead of myself.

P.s. If you are going to pickup any books on mathematics, I **highly** recommend Trudeau's [Intro to Graph Theory](http://www.amazon.com/Introduction-Graph-Theory-Dover-Mathematics/dp/0486678709) on Dover Publications. Its a phenomenal intro to pure math and one of its most enjoyable branches.

# Table of Contents

Chapter 0. **Why Rigour Makes a Difference**  
Chapter 1. **Basic Formal Logic**  
Chapter 2. **Quantifiers**  
Chapter 3. **Sets and Diagrams**


# Chapter 0: Why Rigour Makes a Difference

It was the during the Spring of 1901 that a young mathematician named Bertrand Russell stumbled upon a discovery that would pose a very serious threat to the work done in his field. Russel, a sort of polymath, had been working on *The Principles of Mathematics*, a book whose prime focus would be to demonstrate that mathematics and formal logic were in fact identical entities. He had studied the foundations of mathematics at Trinity College, Cambridge, and had become convinced during a visit to the International Congress of Philosophy, held in Paris, that a logical reform was needed regarding the philosophy of Mathematics$^1$. Naturally, Russel's work would involve the idea of sets and set theory; many of the ideas central to set theory are of critical importance to mathematics to this day. The field was not void of its challenges: paradoxes regarding sets had been discovered not long before the turn of the 20th century. In 1897 the Italian mathematician Cesare Burali-Forti had discovered a [paradox](http://mathworld.wolfram.com/Burali-FortiParadox.html) by which no set could contain all ordinal numbers - a concept which had been introduced by Georg Cantor just a few decades before. Russel had been studying Cantor's work as well, though his discovery would prove to be an extremely powerful and remarkably simple paradox that would disrupt all of Set Theory. What's more, unlike other logical impossibilities before it, Russel's paradox arose solely from the simple definition used to describe sets themselves.

There are a few ways to explain the discovery. The most intuitive version describes the problem with an analogy as follows:

*"There is a town in which a barber lives. The barber solely shaves each man who does not shave himself. If this is the case, who shaves the barber?"*

The paradox is easy to spot. If the barber shaves himself, then he will have broken his own rule by shaving a man who shaves himself. If he must not shave himself, however, he is then forced by his own rule again to shave himself, as he now fits the criteria. This chain of reasoning can continue endlessly, hence the paradox.

The formal explanation of the problem in only a little bit more involved. Consider the following definition:

<h4 style="text-align:center"> <em> A set is a collection of zero or more distinct objects.</em></h4>

This may seem a bit vague, and it is somewhat intended to be. A set is a collection, and the objects can be anything: numbers, letters, events, people, whatever. There is one type of member, however, that leads to Russel's paradox. The explanation is as follows:

We will consider a special variety of sets, ones which contain *themselves* as an element. These sets are called *extraordinary* sets, and resemble the form:

$$A = \{1, 2, 3, ..., A\}$$

Any other set that does not contain itself as an element is thus called an *ordinary* set. To relate these concepts to the barber analogy, all ordinary sets are equivalent to the men who shave themselves, and all extraordinary sets the men that do not.

Now, we consider one very specific set, who acts as the barber. This set, we'll call it $P$, is defined as the set of all ordinary sets. Much like how we ask who shaves the barber, here we ask ourselves whether $P$ is an ordinary or extraordinary set. With some reasoning, it follows that it can be neither. If $P$ is an ordinary set, then it would need to be included within itself, as $P$ is composed of all ordinary sets. If $P$ became a member of itself, however, it then becomes an extraordinary set, and so we must remove it. This action returns $P$ to the status of ordinary, and so on. This chain of reasoning may continue endlessly, hence the paradox.

Russel wasn't fully aware of the ramifications of his discovery, and was quick to write about it both in his book and in various letters to other mathematicians. Luckily, most work would not be disrupted, as the flaw could be amended which a small change to the definition of a set:

<h4 style="text-align:center"> <em>A set is a collection of distinct objects, except for the set itself.</em></h4>

Mathematicians were careful before, but they've been on their toes ever since. This kind of paradox is precisely why we must always be so rigorous when we do Mathematics. Mathematicians have come to a sort of universal agreement that we must always be skeptical until we are fully convinced of ideas, lest they contain inherent flaws like the one Russel came up with. It seems over the top in some scenarios, but it is of utmost importance that substantial rigor be provided whenever an argument is made. Some students of Mathematics and Computer Science become confused by this: they believe that there is a particular way to write, or a series of steps to follow, in order to achieve perfect rigor. In fact, this is impossible. With such a wide variety of problems, it is more important that students recognize that a rigorous argument is one that cannot be broken. When reading a proof, no one should be able to poke holes in your argument, and your points should flow logically from one to the other. It will be difficult to understand this at first, but as you see more examples, things will become more clear. We can never be sure that another Bertrand Russel will show up to clean up after us, so its best to remain on our guard.

Sources:

1. Russel, Bertrand. "Letter to Frege." Letter to Gottlob Frege. 1902. MS. [source](http://isites.harvard.edu/fs/docs/icb.topic1219929.files/FregeRussellCorr.pdf), n.p.

2. Trudeau, Richard J. "Introduction to Graph Theory." Dover Publications.

# Chapter 1: Basic Formal Logic

Before we can begin to tackle Discreet Mathematics and Computer Science, it's important that we first understand one of their supporting entities: formal logic.

People have devoted an entire field of study to logic; their primary focuss is to examine ***propositions***,
***statements*** and ***deductive arguments***. The logic they practice lends itself well to programming and mathematics, as the
paradigms employed in those fields are what most people would consider to be "logical". As we'll see, most people already have a fairly good understanding of logic, but need to refine their understanding. If you don't feel like you have one, don't worry, we'll start from the basics.

The decision to use formal logic in mathematics is debated to some extent, although the large majority of people agree that the principles and ideas we'll be visiting shortly are ones worth adopting. And since this tends to come quickly to many people, many students like to claim that the concepts described here are largely common sense. I would advice you not to jump ahead of yourself if you feel this way - our idea of common sense tends to range vastly from person to person.  We'll be explicit so as to avoid confusion; courses in discreet mathematics often define logical concepts concretely from the beginning, much as we will do here.

You might be wondering why logic matters at all. Well, as I'm sure you already know, English isn't a particularily
accurate language. Unfortunately, many problems in both Computer Science and Mathematics require us to be very precise, which is why we must train ourselves to be specific. This can help us word our problems clearly, and analysize others quickly when English has tendency to fall short. If you grew up in the North America, you were probably exposed to "word problems" in elementary school math classes. These problems typically attempt to relate math to the real world. Here are a few examples:

*A car and a bus set out at 2 p.m. from the same point, each headed in the same direction. The average speed of the car is
30mph slower than twice the speed of the bus. In two hours, the car is 20 miles ahead of the bus; determine the speed at
which the car is travelling.*

Word problems often annoy people because they force them to spend time deciphering what is truly being asked of them before they can actually apply their knowledge. While it's obviously true that we adapt the real world to our more precise mathematical models, a lot of these questions do no more than add pointless context to the theorems we actually care about. Thankfully, the formal logic we'll discuss can also help us solve word problems by deducing their core meaning, and can help us create accurate summaries of the problems we face in our day to day.

So, let's start with natural language: full of logical concepts, but altogether a relatively weak medium for communicating them. As examples of English's short commings, consider the following statements:

"Prositutes appeal to the pope."  
"Miners refuse to work after death."  
"New obersity study looks for larger test group."  

Each phrase can be interpreted in multiple ways, and none of their intended meanings can be deciphered once context is removed. Generally speaking, if we want to define our problems, it is best to avoid these kinds of slip ups. 

# Lesson 2: Quantifiers

First and foremost, we need to learn how to quantify statements.

Quantification, as its name suggests, is a way of analyzing a *quantity* of "objects". These "objects" can be anything - numbers, people, buildings, etc; they are simply "things", and there are three general ways to quantify them. Any statement pertaining to objects may refer to *none* of them, *some* of them, or *all* of them. We'll talk about the concept of "none" or "nothingness" later when we relate these ideas to set theory.

As an example, lets consider a class of students. We don't know exactly how many students are in the class, but we don't actually care about that in this situation. There could be 5 students, or 5 million. The nice thing about logic is that - like algebra - we can discuss "properties" of the class without binding ourselves to specifics. Without any tools but quantification, I can convey some facts about the class to you, without making mention of specific students:

- Every student is taking a math course.
- Some of the students are boys.
- None of the students understand quantification.
- Eventually, all students will understand quantification.

Each statement uses one of the three "quantifications" mentioned earlier. Whats more, there are symbols that we can use to refer to them! 

Each of the following symbols represent what's known as a *quantifier*:

*The Universal Quantifier* : $\forall$

$\forall$ is the equivalent to "all" or "everything". When you read the symbol aloud, read it literally as "for all" or "for every". When we use this quantifier, we are refering to the entirety of our context. In the previous example, $\forall$ is used to refer to *every* student in the class. There are lots of ways to discuss for all in english - "all", "every" and "each" refer to the same overarching amount.

Here's how we can rewrite a sentence using the quantifier:

"Every student in the class likes icecream."

$\forall$ student $x$ in the class, $x$ likes icecream.

I've taken a sentence, and turned it into a *statement*.

Notice that we used algebra: this notion of a quantifier is closely related to the concept of substitution. We create a label, $x$, and because we associated it with the universal quantifier, we can replace (substitute) $x$ with any student in the class. 

So, in other words, we look to the beginning of our sentence ("$\forall$ student $x$ in the class") to see how many people ("objects") we're discussing, then look to the end of our sentence ("$x$ likes icecream") for the associated property. 

So, in otherwords, since the quantifier covers every student in the class, we could pick any one of them, and substitute them into "$x$ likes icecream" and the statement would be true.


*The Existential Quantifier* : $\exists$

$\exists$ is the equivalent to "some". When read aloud, read it literally as "there exists" or, more verbosely, "there exists as least one". When we use this quantifier, we are refering to some part of our context. In the class example, $\exists$ is used to refer to some portion of the students in the class. Don't be confused: it doesn't actually matter how many students we're refering to - as long as *at least one of them* satisfies the property at hand, we're happy. 

So, if we have a collection of objects in a statement, like in the phrase "Some of the students are boys", we can conclude that at least one of the students in the class is a boy. There may be more than one boy in the class, but all we know for sure is that some of them are. We cannot conclude that every student in the class is a boy, nor can we conclude that none of the students are boys.

There are a few ideas that arise out of these two quantifiers. For the following claims, I'll use this table, giving some info about four people in an office:


| Name    | Favourite Programming Language | Likes Math |
|:---------:|:--------------------------------:|:------------:|
| Monica  | Python                         | F          |
| Alex    | Ruby                           | F          |
| Jeffery | R                              | T          |
| Lola    | Haskell                        | T          |


If I make a statement that employs a quantifier, we say that the statement is *quantified*. Furthermore, keep in mind that every statement is either true or false. This is one of those axiomatic things that we all just sort of agree on; if you're interested in learning more about it, send me an email.

**To prove that a universal statement is true**, you must show *that every element in your collection* maintains the property it describes.

> **Example: Everyone at the office has a favourite programming language**.

> I verify that none of the entries in the chart have "null" under the category "Favourite Programming Language", and since everyone does indeed have a favourite language, the statment is true.

**To prove that an existential statement is true,** you must find *at least one element* in your collection that maintains the property it describes.

> **Example: At least one of the workers in the office has a name starting with J.**

> "Jeffery" begins with a J, meaning that at least one of our office workers satisfies the property, and therefore the statement is true. Of course, Jeffery is the only one who satisfies this property - a universal quantification would have made the statement false.

As it turns out, Universal and Existential quantifiers are sort of "opposites". To disprove one, we use the other:

**To disprove a universal claim**, you must find *at least one element* in your collection that does not maintain the property it describes.

> **Example: Everyone at the office likes math.**

> Clearly this claim is false, because I can find at least one (two in fact!) people in the office who do not like math. In otherwords, the universal claim is false because *there exists* at least one person who doesn't satisfy the claim. In this case, Monica and Alex are those two people.

**To disprove an existential claim**, you must show that none of the elements in the collection satisfy the property.

> **Example: At least one person at the office has C as their favourite language.**

> Again, this claim is false, because I can't find a single person who's favourite language is C. In otherwords, the universal claim is false because *for all* the employees, none of them satisfy the property.

##### Taking things further: 
Though they're distinct entities, existential claims may be "contained" within Universal ones. Notice that if I make the claim:

"Everyone at the office has a favourite programming language." 

Which we verified as true, I can also make the statement:

"At least one person at the office has a favourite programming language."

Of course, the second statement is also true! It isn't a "proper" quantification, but it is valid nevertheless. So, in other words, the take away is that if we can make a universal claim about a property within our set of objects, we can also make an existential claim about the same property.

Be careful, however, the opposite is **not** true. If I make an existential claim, I may not be able to switch the quantifier. 

# Lesson 3: Sets and Diagrams

Remember when we talked about Bertrand Russel, and his discoveries about sets? As it turns out, Set Theory lends itself extremely well to formal logic. Recall that the *correct* definition of a set is:

<h4 style="text-align:center"> <em>A set is a collection of distinct objects, except for the set itself.</em></h4>

Where the term *object* can refer to, as we discussed, literally anything! So far, we've used English driven examples to explain quantification. Now, we're going to shift our frame of focuss towards more traditional mathematics.

We're going to use the fictional class discussed in chapter 1, although we're going to be a bit more specific. Let's suppose that our class, which we will now treat as a set, is labelled $C$ (the term *labelled* here simply means that I've given it a name, and that the name is $C$). There's no particular reason for that name, I just chose to work with the first letter of the word; we could just have easily named our set *class*, *students* or *setty-mc-setty-face*. Computer Scientists don't tend to like typing more than they have to, so we'll just stick with $C$. 

Let's say that after all this time, it turned out there were only five students in our class. Here's some information about them:

|  Name  | Favourite Programming Language | Likes Math | Does Homework |
|:------:|:------------------------------:|:----------:|:---------------:|
| Jeff   |                R               |      T     |       T       |
|  Chris |                C               |      T     |       F       |
| Humair |                C               |      T     |       T       |
|  Nana  |             Haskell            |      T     |       T       |
| Alie   |             Python             |      F     |       T       |


I can re-represent my set with mathematical notation, as follows:

$$C = \{Jeff, Chris, Humair, Nana, Alie\}$$

We take the variable, $C$, and set it equal to the set, which is collected in braces {}. We don't typically describe those objects properties with any specific mathematical notation - in our case a table is sufficient. 

Now for a few definitions:

1. We say that the set is our ***universe***. That is, objects can either exist within our universe or not.
2. If an object is a member of our set, we say that it is an ***element*** of the set. 
3. Two sets are said to be **equal** if they contain the exact same elements.
4. A **subset** is another set containing only elements of our original set. We say that the original set is a ***superset***.   
Note that, based on this definition, every set is a subset of itself.

  *Proof*: Consider a set, $C$, containing some number of some elements. We define a subset a one that contains some or all of the elements in $C$. Since $C$ contains all the elements of $C$, we conclude, by definition, that $C$ is a subset of $C$.

 That sort of almost silly, rote application of mathematics is a very common thing, we'll see more of it. Also, in terms of notation, if $A$ is a subset of $B$, we write $A \in B$. 

5. There is a special kinda of set, called the ***empty set***, which contains no elements. It is denoted $\emptyset$. Note, by the definition of equivalency of sets, that there is only one empty set, not many. By definition 4, the empty set is a subset of *every* set.

Here are a few pictoral represenations of these concepts:


# Lesson 4: Implication

We've seen implications before in natural language. Whenever we make them, they are typically of the form "If $x$, then $y$." In mathematical notation, we write, $x \implies y$.

Formally, we split that sentence in two parts and label them accordingly. $x$ is said to be the *antecedent* and $y$ is said to be the *consequent*. The important thing to note is that the relationship between $x$ and $y$ is unidirectional. Although $x$ necessarily suggests $y$, the opposite is not necessarily true. That's a bit abstract, so let's take an example:

Suppose I make the following claim:

"If it is raining outside, then I'm wearing a raincoat."

Or, using our notation,

Rain falling outside $\implies$ my wearing of a raincoat

The antecedent is the first part of the setence - the idea of rain falling outside. The consequent is my wearing of a raincoat.

Now, if this statement it true, it means that whenever it rains outside, you can count on the fact that I'll be wearing a raincoat. It **does not**, however, mean that I can't wear a raincoat otherwise. So, if it happens to be sunny outside and I'm wearing a raincoat, my implication is still true.

This is what was meant by the aforementioned claim: my wearing of a raincoat does not depend on the presence of rain at all, however, if it is in fact raining you can count on my wearing of a raincoat.

Here are all the cases which make my implication true:

1. If it is raining, and I wear a raincoat, the implication is true.
2. If it isn't raining, and I wear a raincoat, the implication is true.
3. If it isn't raining, and I'm not wearing a raincoat, the implication is true.

This is the only case where my implication becomes false:

1. If it is raining, and I don't wear a raincoat, the implication is false.

In the last case, I have broken my promise. You see that it is raining, you count on me to wear a raincoat, and I don't. The one situation you expected doesn't hold up. 

In general, we can look at this relationship with antecedents, consequents, and the validity of the implication they belong to:

  Antencedent  | Consequent | Implication | 
|:------------:|:----------:|:----------:|
| True         |   True     |      T     | 
|  True        |   False    |      F     | 
| False        |   True     |      T     | 
|  False       |   False    |      T     | 

Notice that only in the second case does the implication  become false. 

# Lesson x: Proofs 

Having made our way through the basics of formal logic and set theory, we are now prepared to tackle the real foundations of all mathematics: proofs. Actually, that isn't quite true. Theorems are the real foundations of mathematics, and proof is a tool we employ to discover theorems. 

You really shouldn’t be afraid of proofs (many people tend to be at first), they’re much more natural than you think. We just need to make sure that you learn the “rules of the game” so that you can assure yourself that your proofs are correct.

Proofs, ideally, should not bend to intuition. In other words, just because something seems odd, or perhaps unintuitive, doesn’t necessarily mean that you should suspect your proof of error (we'll see a very famous example shortly). That isn't to say that you should discard your intuition - it can be extremely useful. It just means that, first and foremost, you should rely on definitions, axioms, and other theorems as the tools by which you will prove new results. 

I think this is something a lot of new students fail to see: proving theorems is like playing a game, where you arrange facts that you already know to be true into new discoveries. We generally present proofs in a retrospective tone ("*Prove* that $x$ is true", "*Show* $y$ to be true", "Use definition $p$ to *illustrate* theorem $q$") - this gives students the impression that the answer is there, and they need to just magically come up with it. In reality, mathematicians spend most of their time discovering things by simply re-arranging what they already know. If that's how they discover results, it only makes sense that you should follow their example.

#### A three step approach to proofs

Now, obviously, we won't always be given nice pieces of the puzzle that we can use to prove results. Often, we will be left with a statement. This is why I like to break most proofs down into three steps:

1. *Understand what you are trying to show.*

	I cannot stress this enough. You’d be amazed how many people suffer in university level courses simply because they haven’t bothered to understand what is being asked of them. If your first year linear algebra teacher asks you to prove that every Span is a Subspace, you better understand the terms “Span” and “Subspace” before you can attempt the proof. 

	Once you understand the question, you can begin to ponder what information may be useful to you. These could be definitions, axioms or other theorems. You'll need to keep an open mind.

2. *Understand why the statement you seek to prove is true.*

	This is the tricky part, but also the most rewarding: seeing the answer, and understanding why it makes sense. Sometimes, this can be as easy as “plugging and chugging” those axioms, definitions and other theorems from before until you get to a result, but this is seldom the case. You'll need to explore every avenue you can and re-arrange all the puzzle pieces until you see the answer. Sometimes, even once you've actually got an answer, you'll need to go through it again to understand why it actually makes sense. 
	
3. *Write up the proof so that everyone else can follow it.*

	This is sometimes surprisingly involved. Many proofs read like essays, others read like a set of instructions. Regardless of the format, you must make sure that there are absolutely no holes or jumps in your logic. Every sub-idea of your proof must flow logically from one to the next, otherwise your solution will be flawed and your proof rejected. 

We’re going to run through this process with a few examples, highlighting key points about each step along the way so that you can get a better sense of them.
 
#### **Example 1: Prove that the square of an even number is also even.**

***1. Understanding what we are trying to show.***

This example is fairly algebraic, so we’re going to need a few definitions. First we ask ourselves, do we understand the question? We need to examin the *squares* of *even* numbers. Let us start by writing down those two deinitions:

**Definition 1: The Square of a Number**  
The square of the number is achieved by multiplying the number with itself once; in other words, by raising it to a power of two.

The general form of a square number looks like this:

Given a number, *x*, it's square is denoted as $x * x = x^2$ 

> ex1: The square of 5 is 5x5 = 25  
> ex2: The square of 9 is 9x9 = 81

**Definition 2: Even Numbers**  
An even number is defined as an integer that can be divided by 2 with no remainder. 

More formally, an integer $i$ is even if $\exists$ another integer, $k$, such that:

$$i = 2k$$

That one might seem like a little bit of a jump, so I’ll show you how I got it there:

First, we agree that an even number is divisible by 2 with no remainded. In other words, we agree that if I have a number, $i$, and I divide it by 2, I get a new number, $k$, which has no remainder. 

$${i \over 2} = k$$

Using basic algebra, I can rearrange this expression:

$${i \over 2} = k$$
$$(2){i \over 2} = (2)k$$
$$i = 2k$$

And thus I have shown that even numbers are of the form $i = 2k$. 

***2. Understanding why our claim is true.***

With our definitions in place, we can begin to tackle the problem. As I mentioned, this one is fairly algebraic, and as it turns out we can just apply one definition to the other. I know that I’m going to start off with an even number. Again, let’s call that even number $i$. We don't know what value $i$ represents specifically, all we know is that it is an integer.

My first fact is as follows: $i$ can be written of the form $i = 2k$ (where $k$ is some integer), by definition 2.

I want to see what happens when I square $i$. It wouldn't be very helpful to just write $i^2$, so instead i’m going to use my more verbose version: 

$$i = 2k $$
$$i^2 = (2k)^2$$
$$i^2 = 2^2 * k^2$$ 
$$i^2 = 4k^2$$ 

Now I’m at a bit of a standstill, so I revisit my goal. I want to show that $i^2$ is even, meaning that I want to be able to write that $i^2 = 2j$, where $j$ is some integer (it does **not** have to be the same integer as $k$). 

With a little bit of rearranging, I realize that I can divide the 4 into 2s and write this:

$$i^2 = (2)(2)(k^2)$$ 
$$i^2 = (2)(2k^2)$$

This is very close to my goal! The only problem is that I have $2k^2$, instead of a simple singular variable, like in $2j$. With a bit of reasoning however, I manage to see the answer: the specific value of $j$ is actually irrelevant to the definition of and even number. I just need to show that $2k^2$ is an integer, and my proof is done! 

So, I ask myself: is $2k^2$ an integer? Of course! Although I don’t know what value $k$ holds, I know that it’s an integer by assumption, so in other words $2k^2$ is just three integers being multiplied ($2 * k * k)$. Since I know that the integers are closed under multiplication*, I can conclude that $2k^2$ is an integer.

With that, my proof is complete. I let a new letter, $b$, to represent this new integer, rewriting my whole statement as

$$i^2 = 2b$$

Where $b \in \mathbf{Z}$ is some integer. This proves my claim: I managed to show, assuming nothing more than the fact that $i$ is even, that the square of an even number will always be even. In doing this I’ve maintained generality, in other words I’ve shown things abstractly enough (using letters that represent integers rather than actual values) that the property is true. You could plug in any value for $i$ and see that the proof holds. 

\* When I say "operation x is closed under y", I mean that the result of the operation will be consistent with the type of numbers it involves. In much simpler terms, "multiplication is closed under the integers" just means that multiplying integers together will just give you another integer.

***3. Writing up the proof.***

Of course, I did just prove the result, but I was much more verbose than I needed to be. I could have written everything much more simply:

Assume that we have some integer, $i$, such that i is an even number.

This means that there exists an integer, $k$, such that $i = 2k$.

Squaring $i$ gives:

$$i^2 = (2k)^2$$
$$i^2 = 4*k^2$$
$$i^2 = (2)(2k^2)$$
$$i^2 = 2b$$  

Where $b$ is another integer (integers are closed under multiplication), and $b = 2k^2$. This shows that $i^2$ is also of the form for an even number, and so we have proven that all squares of even numbers are also even. 

#### **Example 2: Prove that the square root of 2 is an irrational number.**

This is an extremely famous problem that deserves some context. An it's going to challenge our intuition.

Suppose that we play (a very dull) game, where I show up to you with two wooden planks in my hands. Each plank is easily measurable (like a meter stick). All you need to do is cut a third piece of wood that can serve as a common measure for my two sticks. If you have cut a succesful common measure, it means that we can lay down a bunch of them on each stick such that it fills each of them evenly. In other words, each of the two plank lengths must be divisible by the common measure. 

An example: I bring a plank of length 6 and another of length 16; obviously, 2 is a common measure. 

Generally speaking, I can always get a common measure by simply dividing one number into the other. One assume that you can always take a common measure small enough to get an answer.

Conversely, we might claim that we can always pick two stick lengths to generate *any* common measure possible. This seems fairly intuitive to us, and a carpenter would certainly agree. I mean, why not? 

As it turns out, a very famous group of mathematical hipsters also believed this to be true, way back in the 5th or 6th century BCE. They were known as the Pythagoreans, and they were quite brilliant. Actually, their work was so secretive that a legend about them has developped, centering around the idea that they would [literally kill you](http://michaelgr.com/2008/11/15/just-be-glad-you-arent-pythagoras-student/) for revealing their discoveries to the public. 

Anyways, these crazy greek cult members were staring at our problem of a common measure. They may not have used carpentry as an example, but they certainly were at a standstill. Specifically, they had drawn a picture similar to this one:


![square root of 2](http://mathworld.wolfram.com/images/eps-gif/PythagorassConstant_1000.gif)

The hypotenuse was troubling them. Somehow, they had come believe that no two values could be divided so as to produce a "common measure" of $\sqrt{2}$. 

We won't bother discussing how they got to this point, we're actually going to do the proof using our three guidelines and show that they were in fact correct. 

